{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Vision Transformers (ViT)\n",
    "\n",
    "TOC:\n",
    "- Attention, Self Attention and Multi-head Self Attention mechanisms\n",
    "- Transformers\n",
    "- Vision Transformers (ViT) paper by Google Research\n",
    "- (Optional) Deit paper by FAIR\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "- Official implementation with Jax/Flax by Google: https://github.com/google-research/vision_transformer\n",
    "- Paper:  https://arxiv.org/abs/2010.11929\n",
    "- One of PyTorch implementations:  https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "\n",
    "\n",
    "![img](https://tse2.mm.bing.net/th?id=OIP.cyCA4XEM1F4ueNS2ADCa9wAAAA&pid=Api) | ![im](https://raw.githubusercontent.com/google-research/vision_transformer/master/figure1.png)\n",
    "---|---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention, Self Attention and Multi-head Self Attention mechanisms\n",
    "\n",
    "#### Attention mechanism\n",
    "\n",
    "Refs:\n",
    "- https://d2l.ai/chapter_attention-mechanisms/index.html\n",
    "- https://d2l.ai/chapter_attention-mechanisms/multihead-attention.html\n",
    "- https://d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html\n",
    "\n",
    "The attention mechanism is based on a trainable associative memory with (key, value) vector pairs.\n",
    "\n",
    "- Sequence of N query vectors: $Q \\in \\mathbb{R}^{N \\times d}$ matched vs\n",
    "- Set of $k$ key vectors: $K \\in \\mathbb{R}^{k \\times d}$.\n",
    "- These inner products are then scaled and normalized with a softmax function to obtain $k$ weights.   \n",
    "- The output of the attention is the weighted sum of a set of $k$ value vectors (packed into $V \\in \\mathbb{R}^{k \\times v}$.\n",
    "\n",
    "$$\n",
    "Attention(Q,K,V) = Softmax(Q K^T / \\sqrt{d}) V \\in \\mathbb{R}^{N \\times v}\n",
    "$$\n",
    "\n",
    "with dropout on attention weights.\n",
    "\n",
    "\n",
    "A more generalized form of nonparametric attention pooling:\n",
    "$$\n",
    "f(q) = \\sum_{i=1}^{k}\\alpha(q, k_i) v_i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "k = 4\n",
    "d = 3\n",
    "N = 5\n",
    "\n",
    "\n",
    "Q = (torch.rand(N, d) > 0.7).float()\n",
    "K = torch.eye(k, d)\n",
    "V = torch.arange(k * d, dtype=torch.float32).reshape(k, d) * 4.5\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  4.5000,  9.0000],\n",
       "        [13.5000, 18.0000, 22.5000],\n",
       "        [27.0000, 31.5000, 36.0000],\n",
       "        [40.5000, 45.0000, 49.5000]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3726, 0.2091, 0.2091, 0.2091],\n",
       "        [0.3202, 0.1798, 0.3202, 0.1798],\n",
       "        [0.2091, 0.3726, 0.2091, 0.2091],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.softmax(Q @ K.transpose(0, 1) / math.sqrt(d), dim=1)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16.9410, 21.4410, 25.9410],\n",
       "        [18.3538, 22.8538, 27.3538],\n",
       "        [19.1470, 23.6470, 28.1470],\n",
       "        [20.2500, 24.7500, 29.2500],\n",
       "        [20.2500, 24.7500, 29.2500]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self Attention\n",
    "\n",
    "Query,  key  and  values  matrices are  themselves  computed  from  a  sequence  of N input  vectors: $X \\in \\mathbb{R}^{N \\times D}$\n",
    "\n",
    "- $Q = X W_{Q} \\in \\mathbb{R}^{N \\times d}$\n",
    "- $K = X W_{K} \\in \\mathbb{R}^{N \\times d}$\n",
    "- $V = X W_{V} \\in \\mathbb{R}^{N \\times d}$\n",
    "\n",
    "Attention (recall):\n",
    "$$\n",
    "Attention(Q,K,V) = Softmax(Q K^T / \\sqrt{d}) V \\in \\mathbb{R}^{N \\times v}\n",
    "$$\n",
    "\n",
    "Self-Attention (single head):\n",
    "\n",
    "$$\n",
    "Head = SelfAttention(X, \\{W_{Q}, W_{K}, W_{V}\\}) = Softmax(X W_{Q} (X W_{K})^T / \\sqrt{d}) (X W_{V}) \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3]), torch.Size([5, 3]), torch.Size([5, 3]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 5\n",
    "D = 4\n",
    "d = 3\n",
    "\n",
    "x = torch.rand(N, D)\n",
    "W_q = torch.rand(D, d)\n",
    "W_k = torch.rand(D, d)\n",
    "W_v = torch.rand(D, d)\n",
    "\n",
    "Q = x @ W_q\n",
    "K = x @ W_k\n",
    "V = x @ W_v\n",
    "Q.shape, K.shape, V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.functional import dropout\n",
    "\n",
    "W = dropout(torch.softmax(Q @ K.transpose(0, 1) / math.sqrt(d), dim=1))\n",
    "(W @ V).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-head Self Attention\n",
    "\n",
    "- Head is a single self attention layer\n",
    "- Heads are concatenated\n",
    "- Output linear transform applied\n",
    "\n",
    "\n",
    "$$\n",
    "MultiHeadAttention(X, \\{W^{0,1...h-1}_{Q}, W^{0,1...h-1}_{K}, W^{0,1...h-1}_{V}\\}) = Concat(Head_0, Head_1, ..., Head_{h-1}) W_{O}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 8]), torch.Size([5, 8]), torch.Size([5, 8]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 5\n",
    "D = 6\n",
    "d = 4\n",
    "o = 3\n",
    "\n",
    "num_heads = 2\n",
    "\n",
    "x = torch.rand(N, D)\n",
    "W_qkv = torch.rand(D, 3 * num_heads * d)\n",
    "W_o = torch.rand(num_heads * d, o)\n",
    "\n",
    "Q, K, V = (x @ W_qkv).chunk(3, dim=1)\n",
    "Q.shape, K.shape, V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = dropout(torch.softmax(Q @ K.transpose(0, 1) / math.sqrt(d), dim=1))\n",
    "heads = W @ V\n",
    "heads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(heads @ W_o).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-head Self Attention in Vision\n",
    "\n",
    "\n",
    "- Input images to patches : `(B, C, H, W) -> (B, H // p * W // p, D)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 32, 32]) -> torch.Size([4, 64, 48])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "patch_size = 4\n",
    "embed_dim = 3 * patch_size * patch_size\n",
    "conv = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "batch_images = torch.rand(4, 3, 32, 32)\n",
    "patches = conv(batch_images)\n",
    "patches = patches.flatten(start_dim=2)\n",
    "patches = patches.transpose(1, 2)\n",
    "print(batch_images.shape, \"->\", patches.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- multi-head attention ops:\n",
    "\n",
    "```python\n",
    "num_heads\n",
    "head_dim = embed_dim // num_heads\n",
    "\n",
    "x, (B, N, embed_dim) -> \n",
    "            -> Q = X * W_q, (B, num_heads, N, head_dim) -> \n",
    "            -> K = X * W_k, (B, num_heads, N, head_dim) ->\n",
    "            -> V = X * W_v, (B, num_heads, N, head_dim) ->\n",
    "            -> A = softmax(Q @ K^t * scale), (B, num_heads, N, N) -> Dropout(A) ->\n",
    "            -> H = A @ V, (B, num_heads, N, head_dim) ->\n",
    "            -> y = H @ W_o, (B, N, embed_dim) ->\n",
    "            -> output\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionAttention(nn.Module):\n",
    "    \"\"\"Vision Multi-Head Attention layer with trainable parameters:\n",
    "    - W_q, W_k, W_k : embed_dim * head_dim * num_heads * 3\n",
    "    - W_o : head_dim * num_heads * embed_dim\n",
    "    \n",
    "    .. code-block:: text\n",
    "\n",
    "        x, (B, N, embed_dim) -> \n",
    "                    -> Q = X * W_q, (B, num_heads, N, head_dim) -> \n",
    "                    -> K = X * W_k, (B, num_heads, N, head_dim) ->\n",
    "                    -> V = X * W_v, (B, num_heads, N, head_dim) ->\n",
    "                    -> A = softmax(Q @ K^t * scale), (B, num_heads, N, N) -> Dropout(A) ->\n",
    "                    -> H = A @ V, (B, num_heads, N, head_dim) ->\n",
    "                    -> y = H @ W_o, (B, N, embed_dim) ->\n",
    "                    -> output\n",
    "                    \n",
    "    https://github.com/google/flax/blob/master/flax/nn/attention.py\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads=8, qkv_bias=True, attn_drop=0.):\n",
    "        super().__init__()\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.num_heads = num_heads        \n",
    "        self.scale = float(head_dim) ** -0.5\n",
    "        self.qkv = nn.Linear(embed_dim, num_heads * head_dim * 3, bias=qkv_bias)\n",
    "        self.att_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(num_heads * head_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, _ = x.shape\n",
    "        # (B, N, num_heads * head_dim * 3) -> (B, N, num_heads, head_dim * 3) ->\n",
    "        # -> (B, num_heads, N, head_dim * 3) -> (B, num_heads, N, head_dim) x 3\n",
    "        qkv = self.qkv(x).reshape(B, N, self.num_heads, -1).transpose(1, 2)\n",
    "        Q, K, V = qkv.chunk(3, dim=-1)\n",
    "        attention = (Q @ K.transpose(-1, -2)) * self.scale\n",
    "        attention = attention.softmax(dim=-1)\n",
    "        attention = self.att_drop(attention)\n",
    "        heads = attention @ V\n",
    "        # Heads is (B, num_heads, N, head_dim) => (B, N, num_heads * head_dim)\n",
    "        heads = heads.transpose(1, 2).reshape(B, N, -1)\n",
    "        output = self.proj(heads)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 6\n",
    "att = VisionAttention(embed_dim, num_heads=num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 6, 64, 8]),\n",
       " torch.Size([4, 6, 64, 8]),\n",
       " torch.Size([4, 6, 64, 8]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Q, K, V\n",
    "B, N, _ = patches.shape\n",
    "qkv = att.qkv(patches).reshape(B, N, att.num_heads, -1).transpose(1, 2)\n",
    "Q, K, V = qkv.chunk(3, dim=-1)\n",
    "Q.shape, K.shape, V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 64, 64])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute heads\n",
    "attention = (Q @ K.transpose(-1, -2)) * att.scale\n",
    "attention = attention.softmax(dim=-1)\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 6, 64, 8])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads = attention @ V\n",
    "heads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 64, 48]), True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads = heads.transpose(1, 2).reshape(B, N, -1)\n",
    "output = att.proj(heads)\n",
    "output.shape, output.shape == att(patches).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "\n",
    "Refs:\n",
    "- https://d2l.ai/chapter_attention-mechanisms/transformer.html\n",
    "\n",
    "\n",
    "In NLP Transformers is composed of an encoder and a decoder. \n",
    "The input (source) and output (target) sequence embeddings are added with positional \n",
    "encoding before being fed into the encoder and the decoder that stack modules based on self-attention.\n",
    "\n",
    "<div style=\"background: white;\">\n",
    "<img src=\"https://d2l.ai/_images/transformer.svg\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Vision Transformer for classification](https://arxiv.org/abs/2010.11929)\n",
    "\n",
    "![im](https://raw.githubusercontent.com/google-research/vision_transformer/master/figure1.png)\n",
    "\n",
    "Inspired  by  the  Transformer  scaling  successes  in  NLP, authors  experiment  with  applying  a  standard Transformer directly to images, with the fewest possible modifications. To do so, they split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. Image patches are treated the same way as tokens (words) in an NLP application. They train the model on image classification in supervised fashion.\n",
    "\n",
    "3 types of models:\n",
    "- ViT-Base, 86M params (vs ResNet50, 23M params)\n",
    "- ViT-Large, 307M params\n",
    "- ViT-Huge, 632M params (vs ResNet152x4 from [Big Transfer](https://arxiv.org/abs/1912.11370), 936M)\n",
    "\n",
    "ImageNet (1.3M images): models give modest accuracies of a few percentage points below ResNets of comparable size. Transformers **lack some of the inductive biases** inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.\n",
    "\n",
    "Larger datasets (14M-300M images): large scale training trumps inductive bias. Vision Transformer (ViT) attains \"excellent\" results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. \n",
    "\n",
    "When pre-trained on the public ImageNet-21k dataset or Google's JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. \n",
    "In particular, the best model reaches the accuracy of 88.55% on ImageNet.\n",
    "\n",
    "SOTA on ImageNet: https://paperswithcode.com/sota/image-classification-on-imagenet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement main blocks:\n",
    "- EncoderBlock: layer norms, visual attention and mlp\n",
    "\n",
    "- Visual Transformer: Patchs and Position Embedding + EncoderBlocks + classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"The positionwise feed-forward network or MLP.    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, drop_rate=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"ViT EncoderBlock\n",
    "    \n",
    "    https://github.com/google-research/vision_transformer/blob/9dbeb0269e0ed1b94701c30933222b49189aa33c/vit_jax/models.py#L94\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4, drop_rate=0., attn_drop_rate=0.):\n",
    "        super().__init__()\n",
    "        self.lnorm1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.lnorm2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.attention = VisionAttention(\n",
    "            embed_dim, num_heads=num_heads, attn_drop=attn_drop_rate\n",
    "        )\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.mlp = MLP(embed_dim, int(embed_dim * mlp_ratio), drop_rate=drop_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.lnorm1(x)\n",
    "        y = self.attention(y)\n",
    "        y = self.dropout(y)\n",
    "        x = x + y\n",
    "        \n",
    "        y = self.lnorm2(x)\n",
    "        y = self.mlp(y)\n",
    "        return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 48])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = EncoderBlock(embed_dim, num_heads)\n",
    "out = block(patches)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Positional encoding\n",
    "\n",
    "All tokens are processed at once without any spatial relationship.\n",
    "The idea is to add a learnable parameter to the tokens to retain positional information.\n",
    "\n",
    "According to the paper, they use 1d positional embedding, considering the inputs as a sequence of patches in the raster order.\n",
    "Authors have not observed significant performance gains from using more advanced 2D-aware position embeddings.\n",
    "\n",
    "$$\n",
    "x = x + pe\n",
    "$$\n",
    "where $pe$ is randomly initialized with using Normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"VisionTransformer model\n",
    "    \n",
    "    https://github.com/google-research/vision_transformer/\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        patch_size=16,\n",
    "        hidden_size=768,\n",
    "        input_channels=3,\n",
    "        input_size=224,\n",
    "        num_classes=1000,\n",
    "        num_layers=12,\n",
    "        num_heads=12,\n",
    "        mlp_dim=3072,\n",
    "        drop_rate=0.1, \n",
    "        attn_drop_rate=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_embed = nn.Conv2d(\n",
    "            input_channels, hidden_size, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_size))\n",
    "        \n",
    "        num_patches = (input_size // patch_size) ** 2\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, hidden_size))\n",
    "        self.pos_dropout = nn.Dropout(p=drop_rate)  \n",
    "    \n",
    "        # Define encoder blocks\n",
    "        kwargs = {\n",
    "            \"embed_dim\": hidden_size,\n",
    "            \"num_heads\": num_heads,\n",
    "            \"mlp_ratio\": mlp_dim / hidden_size,\n",
    "            \"drop_rate\": drop_rate,\n",
    "            \"attn_drop_rate\": attn_drop_rate,\n",
    "        }\n",
    "        blocks = [EncoderBlock(**kwargs) for _ in range(num_layers)]\n",
    "        self.blocks = nn.Sequential(*blocks)        \n",
    "        self.lnorm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "\n",
    "        self.mlp_head = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def features(self, x):\n",
    "        patches = self.patch_embed(x)\n",
    "        patches = patches.flatten(start_dim=2)\n",
    "        patches = patches.transpose(1, 2)\n",
    "        \n",
    "        batch_size = patches.shape[0]\n",
    "\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat([cls_token, patches], dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_dropout(x)\n",
    "\n",
    "        x = self.blocks(x)\n",
    "        x = self.lnorm(x)\n",
    "        \n",
    "        # Return the first token\n",
    "        return x[:, 0, ...]        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        f = self.features(x)\n",
    "        y = self.mlp_head(f)        \n",
    "        return y\n",
    "    \n",
    "\n",
    "def vit_b16(num_classes=1000, input_channels=3, input_size=224):\n",
    "    return VisionTransformer(\n",
    "        num_classes=num_classes,\n",
    "        input_channels=input_channels,\n",
    "        input_size=input_size,\n",
    "        patch_size=16,\n",
    "        hidden_size=768,\n",
    "        num_layers=12,\n",
    "        num_heads=12,\n",
    "        mlp_dim=3072,\n",
    "        drop_rate=0.1, \n",
    "        attn_drop_rate=0.0,        \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1000])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = vit_b16()\n",
    "\n",
    "x = torch.rand(4, 3, 224, 224)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.567656"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([m.numel() for m in model.parameters()]) * 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vit_tiny(num_classes=10, input_channels=3, input_size=32):\n",
    "    return VisionTransformer(\n",
    "        num_classes=num_classes,\n",
    "        input_channels=input_channels,\n",
    "        input_size=input_size,\n",
    "        patch_size=4,\n",
    "        hidden_size=512,\n",
    "        num_layers=4,\n",
    "        num_heads=6,\n",
    "        mlp_dim=1024,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.459762"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = vit_tiny()\n",
    "sum([m.numel() for m in model.parameters()]) * 1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare with [PyTorch-Image-Models](https://github.com/rwightman/pytorch-image-models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def timm_vit_tiny(num_classes=10, input_channels=3):\n",
    "    from functools import partial\n",
    "    import torch.nn as nn\n",
    "    from timm.models.vision_transformer import VisionTransformer as TimmVisionTransformer\n",
    "\n",
    "    return TimmVisionTransformer(\n",
    "        img_size=32, patch_size=4, embed_dim=512, depth=4, num_heads=6, mlp_ratio=2, qkv_bias=True,\n",
    "        num_classes=10, norm_layer=partial(nn.LayerNorm, eps=1e-6)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.47617"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timm_model = timm_vit_tiny()\n",
    "sum([m.numel() for m in timm_model.parameters()]) * 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtimm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0min_chans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmlp_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mqkv_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mqk_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdrop_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mattn_drop_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdrop_path_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhybrid_backbone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnorm_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'torch.nn.modules.normalization.LayerNorm'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_chans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                 \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqkv_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqk_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_drop_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                 \u001b[0mdrop_path_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhybrid_backbone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_dim\u001b[0m  \u001b[0;31m# num_features for consistency with other models\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mhybrid_backbone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHybridEmbed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mhybrid_backbone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_chans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_chans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPatchEmbed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_chans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_chans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnum_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_patches\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_patches\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_drop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_path_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# stochastic depth decay rule\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlp_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqkv_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqkv_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqk_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqk_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_drop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_drop_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# NOTE as per official impl, we could have a pre-logits representation dense layer + tanh here\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m#self.repr = nn.Linear(embed_dim, representation_size)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m#self.repr_act = nn.Tanh()\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# Classifier head\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum_classes\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIdentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mtrunc_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mtrunc_normal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcls_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.8/site-packages/timm/models/vision_transformer.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "timm_model.__init__??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_chans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_chans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mimg_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_2tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mpatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_2tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnum_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch_size\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_patches\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_chans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.8/site-packages/timm/models/vision_transformer.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = timm_model.patch_embed\n",
    "a.__init__??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmlp_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mqkv_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mqk_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mattn_drop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdrop_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mact_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'torch.nn.modules.activation.GELU'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnorm_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'torch.nn.modules.normalization.LayerNorm'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlp_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqkv_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqk_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_drop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                 \u001b[0mdrop_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGELU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqkv_bias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqkv_bias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqk_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqk_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_drop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_drop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj_drop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;31m# NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdrop_path\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIdentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmlp_hidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmlp_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlp_hidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_layer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mact_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.8/site-packages/timm/models/vision_transformer.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = timm_model.blocks[0]\n",
    "a.__init__??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ViT on CIFAR10\n",
    "\n",
    "- On TensorBoard.dev: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train ViT on CIFAR10 with [PyTorch-Ignite](https://github.com/pytorch/ignite)\n",
      "\n",
      "\n",
      "We define ViT models adapted for CIFAR10 images of 32x32 size:\n",
      "- vit_tiny_patch4_32x32 : 32x32 input size and patch of 4 pixels\n",
      "- vit_b4_32x32 : our base ViT reimplementation with 32x32 input size and patch of 4 pixels\n",
      "- vit_b3_32x32 : our base ViT reimplementation with 32x32 input size and patch of 3 pixels\n",
      "- vit_b2_32x32 : our base ViT reimplementation with 32x32 input size and patch of 2 pixels\n",
      "- timm_vit_b4_32x32 : timm reimplementation of base ViT with 32x32 input size and patch of 4 pixels\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!head -10 cifar10/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Supervision \n",
    "\n",
    "With self-supervised pre-training e.g. BERT in NLP, smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, as ignificant improvement of 2% to training from scratch, but still 4% behind supervised pre-training.\n",
    "\n",
    "\n",
    "> We employ the masked patch prediction objective for preliminary self-supervision experiments. Todo so we corrupt 50% of patch embeddings by either replacing their embeddings with a learnable [mask] embedding  (80%),  a  random  other  patch  embedding  (10%)  or  just  keeping  them  as is (10%). This setup is very similar to the one used for language by Devlin et al. (2019).  Finally, we predict the 3-bit, mean color (i.e., 512 colors in total) of every corrupted patch using their respective patch representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway\n",
    "\n",
    "- ViT without any handmade image-specific inductive biases\n",
    "- ViT matches or exceeds SOTA on many image classification datasets, whilst being relatively cheap to pre-train.\n",
    "- Initial experiments show improvement from self-supervised pre-training, but there is still large gap between self-supervised and large-scale supervised pre-training. Finally, further scaling of ViT would likely lead to improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Training data-efficient image transformers & distillation through attention](https://arxiv.org/abs/2012.12877)\n",
    "\n",
    "- https://github.com/facebookresearch/deit\n",
    "- https://ai.facebook.com/blog/data-efficient-image-transformers-a-promising-new-technique-for-image-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformers\n",
    "\n",
    "- https://github.com/google-research/vision_transformer\n",
    "- https://arxiv.org/abs/2010.11929\n",
    "- https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "\n",
    "\n",
    "![img](https://tse2.mm.bing.net/th?id=OIP.cyCA4XEM1F4ueNS2ADCa9wAAAA&pid=Api) | ![im](https://raw.githubusercontent.com/google-research/vision_transformer/master/figure1.png)\n",
    "---|---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention, Self Attention and Multi-head Self Attention mechanisms\n",
    "\n",
    "#### Attention mechanism\n",
    "\n",
    "Refs:\n",
    "- https://arxiv.org/pdf/2012.12877.pdf\n",
    "- https://d2l.ai/chapter_attention-mechanisms/index.html\n",
    "- https://d2l.ai/chapter_attention-mechanisms/multihead-attention.html\n",
    "- https://d2l.ai/chapter_attention-mechanisms/self-attention-and-positional-encoding.html\n",
    "\n",
    "The attention mechanism is based on a trainable associative memory with (key, value) vector pairs.\n",
    "\n",
    "- Sequence of N query vectors: $Q \\in \\mathbb{R}^{N \\times d}$ matched vs\n",
    "- Set of $k$ key vectors: $K \\in \\mathbb{R}^{k \\times d}$.\n",
    "- These inner products are then scaled and normalized with a softmax function to obtain $k$ weights.   \n",
    "- The output of the attention is the weighted sum of a set of $k$ value vectors (packed into $V \\in \\mathbb{R}^{k \\times v}$.\n",
    "\n",
    "$$\n",
    "Attention(Q,K,V) = Softmax(Q K^T / \\sqrt{d}) V \\in \\mathbb{R}^{N \\times v}\n",
    "$$\n",
    "\n",
    "with dropout on attention weights.\n",
    "\n",
    "\n",
    "A more generalized form of nonparametric attention pooling:\n",
    "$$\n",
    "f(q) = \\sum_{i=1}^{k}\\alpha(q, k_i) v_i\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "k = 4\n",
    "d = 3\n",
    "N = 5\n",
    "\n",
    "\n",
    "Q = (torch.rand(N, d) > 0.7).float()\n",
    "K = torch.eye(k, d)\n",
    "V = torch.arange(k * d, dtype=torch.float32).reshape(k, d) * 4.5\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  4.5000,  9.0000],\n",
       "        [13.5000, 18.0000, 22.5000],\n",
       "        [27.0000, 31.5000, 36.0000],\n",
       "        [40.5000, 45.0000, 49.5000]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3726, 0.2091, 0.2091, 0.2091],\n",
       "        [0.3202, 0.1798, 0.3202, 0.1798],\n",
       "        [0.2091, 0.3726, 0.2091, 0.2091],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.softmax(Q @ K.transpose(0, 1) / math.sqrt(d), dim=1)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16.9410, 21.4410, 25.9410],\n",
       "        [18.3538, 22.8538, 27.3538],\n",
       "        [19.1470, 23.6470, 28.1470],\n",
       "        [20.2500, 24.7500, 29.2500],\n",
       "        [20.2500, 24.7500, 29.2500]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Self Attention\n",
    "\n",
    "Query,  key  and  values  matrices are  themselves  computed  from  a  sequence  of N input  vectors: $X \\in \\mathbb{R}^{N \\times D}$\n",
    "\n",
    "- $Q = X W_{Q} \\in \\mathbb{R}^{N \\times d}$\n",
    "- $K = X W_{K} \\in \\mathbb{R}^{N \\times d}$\n",
    "- $V = X W_{V} \\in \\mathbb{R}^{N \\times d}$\n",
    "\n",
    "\n",
    "$$\n",
    "Head = SelfAttention(X, \\{W_{Q}, W_{K}, W_{V}\\}) = Softmax(X W_{Q} (X W_{K})^T / \\sqrt{d}) (X W_{V}) \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3]), torch.Size([5, 3]), torch.Size([5, 3]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 5\n",
    "D = 4\n",
    "d = 3\n",
    "\n",
    "x = torch.rand(N, D)\n",
    "W_q = torch.rand(D, d)\n",
    "W_k = torch.rand(D, d)\n",
    "W_v = torch.rand(D, d)\n",
    "\n",
    "Q = x @ W_q\n",
    "K = x @ W_k\n",
    "V = x @ W_v\n",
    "Q.shape, K.shape, V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.functional import dropout\n",
    "\n",
    "W = dropout(torch.softmax(Q @ K.transpose(0, 1) / math.sqrt(d), dim=1))\n",
    "(W @ V).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-head Self Attention\n",
    "\n",
    "- Head is a single self attention layer\n",
    "- Heads are concatenated\n",
    "- Output linear transform applied\n",
    "\n",
    "\n",
    "$$\n",
    "MultiHeadAttention(X, \\{W^{0,1...h-1}_{Q}, W^{0,1...h-1}_{K}, W^{0,1...h-1}_{V}\\}) = Concat(Head_0, Head_1, ..., Head_{h-1}) W_{O}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 8]), torch.Size([5, 8]), torch.Size([5, 8]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 5\n",
    "D = 6\n",
    "d = 4\n",
    "o = 3\n",
    "\n",
    "num_heads = 2\n",
    "\n",
    "x = torch.rand(N, D)\n",
    "W_qkv = torch.rand(D, 3 * num_heads * d)\n",
    "W_o = torch.rand(num_heads * d, o)\n",
    "\n",
    "Q, K, V = (x @ W_qkv).chunk(3, dim=1)\n",
    "Q.shape, K.shape, V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = dropout(torch.softmax(Q @ K.transpose(0, 1) / math.sqrt(d), dim=1))\n",
    "heads = W @ V\n",
    "heads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(heads @ W_o).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-head Self Attention in Vision\n",
    "\n",
    "\n",
    "- Input images to patches : `(B, C, H, W) -> (B, H // p * W // p, D)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 32, 32]) -> torch.Size([4, 64, 48])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "patch_size = 4\n",
    "embed_dim = 3 * patch_size * patch_size\n",
    "conv = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "batch_images = torch.rand(4, 3, 32, 32)\n",
    "patches = conv(batch_images)\n",
    "patches = patches.flatten(start_dim=2)\n",
    "patches = patches.transpose(1, 2)\n",
    "print(batch_images.shape, \"->\", patches.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- multi-head attention ops:\n",
    "\n",
    "```python\n",
    "x, (B, N, embed_dim) -> \n",
    "            -> Q = X * W_q, (B, N, embed_dim) -> \n",
    "            -> K = X * W_k, (B, N, embed_dim) ->\n",
    "            -> V = X * W_v, (B, N, embed_dim) ->\n",
    "            -> A = softmax(Q @ K^t * scale), (B, N, N) -> Dropout(A) ->\n",
    "            -> H = A @ V, (B, N, embed_dim) -> \n",
    "            -> y = H @ W_o, (B, N, embed_dim) -> Dropout(y) ->\n",
    "            -> output\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionAttention(nn.Module):\n",
    "    \"\"\"Vision Multi-Head Attention layer with trainable parameters:\n",
    "    - W_q, W_k, W_k : embed_dim * embed_dim * 3\n",
    "    - W_o : embed_dim * embed_dim\n",
    "    \n",
    "    .. code-block:: text\n",
    "\n",
    "        x, (B, N, embed_dim) -> \n",
    "                    -> Q = X * W_q, (B, N, embed_dim) -> \n",
    "                    -> K = X * W_k, (B, N, embed_dim) ->\n",
    "                    -> V = X * W_v, (B, N, embed_dim) ->\n",
    "                    -> A = softmax(Q @ K^t * scale), (B, N, N) -> Dropout(A) ->\n",
    "                    -> H = A @ V, (B, N, embed_dim) -> \n",
    "                    -> y = H @ W_o, (B, N, embed_dim) ->\n",
    "                    -> output\n",
    "                    \n",
    "    https://github.com/google/flax/blob/master/flax/nn/attention.py\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads=8, qkv_bias=False, attn_drop=0.):\n",
    "        super().__init__()\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.scale = float(head_dim) ** -0.5\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3, bias=qkv_bias)\n",
    "        self.att_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        Q, K, V = self.qkv(x).chunk(3, dim=-1)\n",
    "        attention = (Q @ K.transpose(1, 2)) * self.scale\n",
    "        attention = attention.softmax(dim=-1)\n",
    "        attention = self.att_drop(attention)\n",
    "        heads = attention @ V\n",
    "        output = self.proj(heads)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 6\n",
    "att = VisionAttention(embed_dim, num_heads=num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 64, 48]), torch.Size([4, 64, 48]), torch.Size([4, 64, 48]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute Q, K, V\n",
    "Q, K, V = att.qkv(patches).chunk(3, dim=-1)\n",
    "Q.shape, K.shape, V.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 64])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute heads\n",
    "head_dim = embed_dim // num_heads\n",
    "attention = (Q @ K.transpose(1, 2)) * float(head_dim) ** -0.5\n",
    "attention = attention.softmax(dim=-1)\n",
    "attention = att.att_drop(attention)\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 48])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heads = attention @ V\n",
    "heads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 64, 48]), True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = att.proj(heads)\n",
    "output.shape, output.shape == att(patches).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "\n",
    "Refs:\n",
    "- https://d2l.ai/chapter_attention-mechanisms/transformer.html\n",
    "\n",
    "\n",
    "In NLP Transformers is composed of an encoder and a decoder. \n",
    "The input (source) and output (target) sequence embeddings are added with positional \n",
    "encoding before being fed into the encoder and the decoder that stack modules based on self-attention.\n",
    "\n",
    "<div style=\"background: white;\">\n",
    "<img src=\"https://d2l.ai/_images/transformer.svg\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Vision Transformer for classification](https://arxiv.org/abs/2010.11929)\n",
    "\n",
    "![im](https://raw.githubusercontent.com/google-research/vision_transformer/master/figure1.png)\n",
    "\n",
    "Inspired  by  the  Transformer  scaling  successes  in  NLP, authors  experiment  with  applying  a  standard Transformer directly to images, with the fewest possible modifications. To do so, they split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. Image patches are treated the same way as tokens (words) in an NLP application. They train the model on image classification in supervised fashion.\n",
    "\n",
    "3 types of models:\n",
    "- ViT-Base, 86M params (vs ResNet50, 23M params)\n",
    "- ViT-Large, 307M params\n",
    "- ViT-Huge, 632M params (vs ResNet152x4 from [Big Transfer](https://arxiv.org/abs/1912.11370), 936M)\n",
    "\n",
    "ImageNet (1.3M images): models give modest accuracies of a few percentage points below ResNets of comparable size. Transformers **lack some of the inductive biases** inherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well when trained on insufficient amounts of data.\n",
    "\n",
    "Larger datasets (14M-300M images): large scale training trumps inductive bias. Vision Transformer (ViT) attains \"excellent\" results when pre-trained at sufficient scale and transferred to tasks with fewer datapoints. \n",
    "\n",
    "When pre-trained on the public ImageNet-21k dataset or Google's JFT-300M dataset, ViT approaches or beats state of the art on multiple image recognition benchmarks. \n",
    "In particular, the best model reaches the accuracy of 88.55% on ImageNet.\n",
    "\n",
    "SOTA on ImageNet: https://paperswithcode.com/sota/image-classification-on-imagenet\n",
    "\n",
    "\n",
    "Self-Supervision (e.g. BERT in NLP) : With self-supervised pre-training, smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, as ignificant improvement of 2% to training from scratch, but still 4% behind supervised pre-training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement main blocks:\n",
    "- EncoderBlock: layer norms, visual attention and mlp\n",
    "\n",
    "- Visual Transformer: Patchs and Position Embedding + EncoderBlocks + classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"The positionwise feed-forward network or MLP.    \n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, drop_rate=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"ViT EncoderBlock\n",
    "    \n",
    "    https://github.com/google-research/vision_transformer/blob/9dbeb0269e0ed1b94701c30933222b49189aa33c/vit_jax/models.py#L94\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4, drop_rate=0., attn_drop_rate=0.):\n",
    "        super().__init__()\n",
    "        self.lnorm1 = nn.LayerNorm(embed_dim)\n",
    "        self.lnorm2 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = VisionAttention(\n",
    "            embed_dim, num_heads=num_heads, attn_drop=attn_drop_rate\n",
    "        )\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.mlp = MLP(embed_dim, int(embed_dim * mlp_ratio), drop_rate=drop_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.lnorm1(x)\n",
    "        y = self.attention(y)\n",
    "        y = self.dropout(y)\n",
    "        x = x + y\n",
    "        \n",
    "        y = self.lnorm2(x)\n",
    "        y = self.mlp(y)\n",
    "        return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 48])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block = EncoderBlock(embed_dim, num_heads)\n",
    "out = block(patches)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Positional encoding\n",
    "\n",
    "All tokens are processed at once without any spatial relationship.\n",
    "The idea is to add a learnable parameter to the tokens to retain positional information.\n",
    "\n",
    "According to the paper, they use 1d positional embedding, considering the inputs as a sequence of patches in the raster order.\n",
    "Authors have not observed significant performance gains from using more advanced 2D-aware position embeddings.\n",
    "\n",
    "$$\n",
    "x = x + pe\n",
    "$$\n",
    "where $pe$ is randomly initialized with using Normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"VisionTransformer model\n",
    "    \n",
    "    https://github.com/google-research/vision_transformer/\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        patch_size=16,\n",
    "        hidden_size=768,\n",
    "        input_channels=3,\n",
    "        input_size=224,\n",
    "        num_classes=1000,\n",
    "        num_layers=12,\n",
    "        num_heads=12,\n",
    "        mlp_dim=3072,\n",
    "        drop_rate=0.1, \n",
    "        attn_drop_rate=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patchs_embed = nn.Conv2d(\n",
    "            input_channels, hidden_size, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_size))\n",
    "        \n",
    "        num_patches = (input_size // patch_size) ** 2\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, hidden_size))\n",
    "        self.pos_dropout = nn.Dropout(p=drop_rate)  \n",
    "    \n",
    "        # Define encoder blocks\n",
    "        kwargs = {\n",
    "            \"embed_dim\": hidden_size,\n",
    "            \"num_heads\": num_heads,\n",
    "            \"mlp_ratio\": mlp_dim / hidden_size,\n",
    "            \"drop_rate\": drop_rate,\n",
    "            \"attn_drop_rate\": attn_drop_rate,\n",
    "        }\n",
    "        blocks = [EncoderBlock(**kwargs) for _ in range(num_layers)]\n",
    "        self.blocks = nn.Sequential(*blocks)        \n",
    "        self.lnorm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        self.mlp_head = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def features(self, x):\n",
    "        patches = self.patchs_embed(x)\n",
    "        patches = patches.flatten(start_dim=2)\n",
    "        patches = patches.transpose(1, 2)\n",
    "        \n",
    "        batch_size = patches.shape[0]\n",
    "\n",
    "        cls_token = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat([cls_token, patches], dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_dropout(x)\n",
    "\n",
    "        x = self.blocks(x)\n",
    "        x = self.lnorm(x)\n",
    "        \n",
    "        # Return the first token\n",
    "        return x[:, 0, ...]        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        f = self.features(x)\n",
    "        y = self.mlp_head(f)        \n",
    "        return y\n",
    "    \n",
    "\n",
    "def vit_b16(num_classes=1000, input_channels=3, input_size=224):\n",
    "    return VisionTransformer(\n",
    "        num_classes=num_classes,\n",
    "        input_channels=input_channels,\n",
    "        input_size=input_size,\n",
    "        patch_size=16,\n",
    "        hidden_size=768,\n",
    "        num_layers=12,\n",
    "        num_heads=12,\n",
    "        mlp_dim=3072,\n",
    "        drop_rate=0.1, \n",
    "        attn_drop_rate=0.0,        \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1000])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = vit_b16()\n",
    "\n",
    "x = torch.rand(4, 3, 224, 224)\n",
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86.540008"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([m.numel() for m in model.parameters()]) * 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vit_tiny(num_classes=10, input_channels=3, input_size=32):\n",
    "    return VisionTransformer(\n",
    "        num_classes=num_classes,\n",
    "        input_channels=input_channels,\n",
    "        input_size=input_size,\n",
    "        patch_size=4,\n",
    "        hidden_size=512,\n",
    "        num_layers=4,\n",
    "        num_heads=6,\n",
    "        mlp_dim=1024,\n",
    "        drop_rate=0.1, \n",
    "        attn_drop_rate=0.0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.470025999999999"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = vit_tiny()\n",
    "sum([m.numel() for m in model.parameters()]) * 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a tiny version on CIFAR10\n",
    "\n",
    "- On TensorBoard.dev: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train ViT on CIFAR10 with [PyTorch-Ignite]()\n",
      "\n",
      "- on 1 or more GPUs\n",
      "- compute training/validation metrics\n",
      "- log learning rate, metrics etc\n",
      "- save the best model weights\n"
     ]
    }
   ],
   "source": [
    "!head -6 cifar10/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize attention maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Training data-efficient image transformers& distillation through attention](https://arxiv.org/abs/2012.12877)\n",
    "\n",
    "- https://github.com/facebookresearch/deit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
